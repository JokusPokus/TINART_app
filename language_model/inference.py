"""
This modules provides functionality to generate natural language in response
to a seed (the so-called question).
"""

from transformers import AutoTokenizer, AutoModelWithLMHead
from typing import Tuple
import sys


def preprocess_question(question: str) -> str:
    """
    Processes a question string by appending an EOS token.

    return: The preprocessed question
    """
    question += " [End_Question]"
    return question


def process_answer(answer: str, end_of_question: int) -> str:
    """
    Processes an answer generated by the language model.
    Cuts off irrelevant and incomplete parts in the beginning and end.

    @return: Processed answer string.
    """
    answer = answer[end_of_question+1:]

    special_tokens = [" [End_Question]", " [End_Answer]"]

    for token in special_tokens:
        while token in answer:
            answer = answer.replace(token, "")

    last_EOS = max(
        answer.rfind("."),
        answer.rfind("!"),
        answer.rfind("?"),
    )
    return answer[:last_EOS+1]


def generate_response(question: str, model: AutoModelWithLMHead, tokenizer: AutoTokenizer) -> str:
    """
    Uses a language model and a tokenizer to create a natural language sample
    in response to a question string.

    @param question:
    @param model: pre-trained (or fine-tuned) language model
    @param tokenizer: pre-trained tokenizer
    @return: processed answer string
    """
    question = preprocess_question(question)
    end_of_question = len(question)
    input_ids = tokenizer.encode(question, return_tensors="pt")

    sample_output = model.generate(
        input_ids,
        do_sample=True,
        max_length=100,
        top_p=0.9,
        top_k=0,
    )
    answer = tokenizer.decode(sample_output[0], skip_special_tokens=True)
    answer = process_answer(answer, end_of_question)

    return answer


def load_model(model_path: str):
    """
    Loads a tokenizer and language model for natural language generation.

    @param model_path: Path that leads to the pretrained (and potentially fine-tuned)
    language model
    @return: A tokenizer and language model for language generation
    """
    tokenizer = AutoTokenizer.from_pretrained("anonymous-german-nlp/german-gpt2")
    model = AutoModelWithLMHead.from_pretrained(model_path)
    return tokenizer, model


def main(politician):
    """
    Opens a dialogue with the language model for testing purposes.
    """
    tokenizer, model = load_model(f".\\gpt2-{politician}\\")

    while True:
        question = input("Your sentence:   ")
        if question == "exit":
            break
        answer = generate_response(question, model=model, tokenizer=tokenizer)
        print("Output:\n" + 100 * '-')
        print(answer)


if __name__ == "__main__":
    test_politician = sys.argv[1]
    main(test_politician)

